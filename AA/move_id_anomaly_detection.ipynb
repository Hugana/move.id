{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1120ac7-8a43-46c1-989f-127e92c674dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paho.mqtt import client as mqtt_client\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from adtk.detector import SeasonalAD\n",
    "from adtk.data import validate_series\n",
    "import pandas as pd\n",
    "from adtk.detector import LevelShiftAD\n",
    "from adtk.detector import QuantileAD\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from adtk.detector import OutlierDetector\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting an item of incompatible dtype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ed80530-fc5d-4f84-8193-16464bad9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_topic_name(topic):\n",
    "    # Replace invalid characters with underscores\n",
    "    sanitized_topic = topic.replace('/', '_')\n",
    "    return sanitized_topic\n",
    "\n",
    "def on_message_data_append(topic, row):\n",
    "    # Sanitize the topic name to ensure it can be used in file paths\n",
    "    csv_file = f\"data_csv_{topic}.csv\"\n",
    "\n",
    "    if not os.path.exists(csv_file):\n",
    "        # If the file does not exist, create it and write the header\n",
    "        with open(csv_file, mode='w', newline='') as file:\n",
    "            file.write('date,x,y,z\\n')\n",
    "    \n",
    "    # Read the CSV file into a DataFrame or create an empty DataFrame if the file is just created\n",
    "    df = pd.read_csv(csv_file) if os.path.exists(csv_file) else pd.DataFrame(columns=['date', 'x', 'y', 'z'])\n",
    "\n",
    "    size = len(df)\n",
    "\n",
    "    # Check the row count and drop the oldest row if necessary\n",
    "    if len(df) >= 120:\n",
    "        df = df.drop(0)\n",
    "\n",
    "    # Create a DataFrame with the new row\n",
    "    new_df = pd.DataFrame([{'date': row[0], 'x': row[1], 'y': row[2], 'z': row[3]}])\n",
    "\n",
    "    # Concatenate the existing DataFrame with the new DataFrame\n",
    "    new_df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    # Save the new DataFrame back to the CSV file\n",
    "    new_df.to_csv(csv_file, index=False)\n",
    "\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f9b07ff-fded-4739-a1c1-705e76ecd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getanomalyList(anomalies):\n",
    "    \n",
    "    # Filter out NaN values from anomalies\n",
    "    non_nan_anomalies = anomalies.dropna()\n",
    "    \n",
    "    anomalies_dates = []\n",
    "    \n",
    "    # Append the dates of the anomalies of the x-axis\n",
    "    for anomaly_idx, anomaly in non_nan_anomalies.items():\n",
    "        if anomaly:\n",
    "            anomalies_dates.append(anomaly_idx)\n",
    "\n",
    "    return anomalies_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c17393-feed-4635-bbe2-6eb9924a37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnomalysLevelShift(topic):\n",
    "    \n",
    "    csv_file = f\"data_csv_{topic}.csv\"\n",
    "    \n",
    "    sleeping_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    sleeping_data['date'] = pd.to_datetime(sleeping_data['date'])\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    sleeping_data.set_index('date', inplace=True)\n",
    "\n",
    "    # get each axis data\n",
    "    x_axis_data = sleeping_data[\"x\"]\n",
    "    y_axis_data = sleeping_data[\"y\"]\n",
    "    z_axis_data = sleeping_data[\"z\"]\n",
    "    \n",
    "    # validating series\n",
    "    x_train = validate_series(x_axis_data)\n",
    "    y_train = validate_series(y_axis_data)\n",
    "    z_train = validate_series(z_axis_data)\n",
    "\n",
    "    # Create and fit the LevelShiftAD detector for x_train\n",
    "    level_shift_ad_x = LevelShiftAD(c=6.0, side='both', window=3)\n",
    "    anomalies_x = level_shift_ad_x.fit_detect(x_train)\n",
    "    anomalies_x_dates = getanomalyList(anomalies_x) # list of anomalies\n",
    "    \n",
    "    # Create and fit the LevelShiftAD detector for y_train\n",
    "    level_shift_ad_y = LevelShiftAD(c=6.0, side='both', window=3)\n",
    "    anomalies_y = level_shift_ad_y.fit_detect(y_train)\n",
    "    anomalies_y_dates = getanomalyList(anomalies_y) # list of anomalies\n",
    "    \n",
    "    # Create and fit the LevelShiftAD detector for z_train\n",
    "    level_shift_ad_z = LevelShiftAD(c=6.0, side='both', window=3)\n",
    "    anomalies_z = level_shift_ad_z.fit_detect(z_train)\n",
    "    anomalies_z_dates = getanomalyList(anomalies_z) # list of anomalies\n",
    "    \n",
    "    return anomalies_x_dates, anomalies_y_dates, anomalies_z_dates\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "126d974f-94a8-4a3e-89ea-e3905b178ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnomalyQuantile(topic):\n",
    "    \n",
    "    csv_file = f\"data_csv_{topic}.csv\"\n",
    "    \n",
    "    sleeping_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    sleeping_data['date'] = pd.to_datetime(sleeping_data['date'])\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    sleeping_data.set_index('date', inplace=True)\n",
    "\n",
    "    # get each axis data\n",
    "    x_axis_data = sleeping_data[\"x\"]\n",
    "    y_axis_data = sleeping_data[\"y\"]\n",
    "    z_axis_data = sleeping_data[\"z\"]\n",
    "    \n",
    "    # validating series\n",
    "    x_train = validate_series(x_axis_data)\n",
    "    y_train = validate_series(y_axis_data)\n",
    "    z_train = validate_series(z_axis_data)\n",
    "\n",
    "    # Create and fit the QuantileAD detector for x_train\n",
    "    quantile_ad_x = QuantileAD(high=0.99, low=0.02)\n",
    "    anomalies_x = quantile_ad_x.fit_detect(x_train)\n",
    "    anomalies_x_dates = getanomalyList(anomalies_x) # list of anomalies\n",
    "\n",
    "    # Create and fit the QuantileAD detector for y_train\n",
    "    quantile_ad_y = QuantileAD(high=0.99, low=0.02)\n",
    "    anomalies_y = quantile_ad_y.fit_detect(y_train)\n",
    "    anomalies_y_dates = getanomalyList(anomalies_y) # list of anomalies\n",
    "\n",
    "\n",
    "    # Create and fit the QuantileAD detector for z_train\n",
    "    quantile_ad_z = QuantileAD(high=0.99, low=0.02)\n",
    "    anomalies_z = quantile_ad_z.fit_detect(z_train)\n",
    "    anomalies_z_dates = getanomalyList(anomalies_z) # list of anomalies\n",
    "\n",
    "    return anomalies_x_dates, anomalies_y_dates, anomalies_z_dates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5735d61b-a7da-404b-9019-3adde10c08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutlierDetection(topic):\n",
    "    \n",
    "    csv_file = f\"data_csv_{topic}.csv\"\n",
    "    \n",
    "    sleeping_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    sleeping_data['date'] = pd.to_datetime(sleeping_data['date'])\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    sleeping_data.set_index('date', inplace=True)\n",
    "\n",
    "    # get each axis data\n",
    "    x_axis_data = sleeping_data[\"x\"]\n",
    "    y_axis_data = sleeping_data[\"y\"]\n",
    "    z_axis_data = sleeping_data[\"z\"]\n",
    "    \n",
    "    # validating series\n",
    "    x_train = validate_series(x_axis_data)\n",
    "    y_train = validate_series(y_axis_data)\n",
    "    z_train = validate_series(z_axis_data)\n",
    "\n",
    "    x_train_df = pd.DataFrame(x_train)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    z_train_df = pd.DataFrame(z_train)\n",
    "    \n",
    "    outlier_detector_x = OutlierDetector(LocalOutlierFactor(contamination=0.02))\n",
    "    anomalies_x = outlier_detector_x.fit_detect(x_train_df)\n",
    "    anomalies_x_dates = getanomalyList(anomalies_x) # list of anomalies\n",
    "    \n",
    "    outlier_detector_y = OutlierDetector(LocalOutlierFactor(contamination=0.02))\n",
    "    anomalies_y = outlier_detector_y.fit_detect(y_train_df)\n",
    "    anomalies_y_dates = getanomalyList(anomalies_y) # list of anomalies\n",
    "   \n",
    "    \n",
    "    outlier_detector_z = OutlierDetector(LocalOutlierFactor(contamination=0.02))\n",
    "    anomalies_z = outlier_detector_z.fit_detect(z_train_df)\n",
    "    anomalies_z_dates = getanomalyList(anomalies_z) # list of anomalies\n",
    "\n",
    "    return anomalies_x_dates, anomalies_y_dates, anomalies_z_dates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47e933b3-fd96-4675-83a3-87e3196057fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anomaly_in_rows(anomaly_timestamps,topic, rows):\n",
    "\n",
    "    # Check if the list is empty\n",
    "    if not anomaly_timestamps:\n",
    "        return False\n",
    "        \n",
    "    df = pd.read_csv(f\"data_csv_{topic}.csv\")\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # Get the last 6 rows of the data\n",
    "    last_rows = df.tail(rows)\n",
    "\n",
    "    # Check if any of the anomaly timestamps exist within the last 6 seconds of the record\n",
    "    for anomaly_timestamp in anomaly_timestamps:\n",
    "        if anomaly_timestamp in last_rows.index:\n",
    "            return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41473d3c-9794-44fa-9b0a-5bb183d9a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_two_true(b1, b2, b3):\n",
    "  # Count the number of True values\n",
    "  true_count = sum([b1, b2, b3])\n",
    "  return true_count >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5ce4dc9-bac6-4948-b6e1-60539b56a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetection(topic,row):\n",
    "    \n",
    "        anomalies_x_dates_ls, anomalies_y_dates_ls, anomalies_z_dates_ls = getAnomalysLevelShift(topic)\n",
    "        anomalies_x_dates_q, anomalies_y_dates_q, anomalies_z_dates_q = getAnomalyQuantile(topic)\n",
    "        anomalies_x_dates_od, anomalies_y_dates_od, anomalies_z_dates_od = getOutlierDetection(topic)\n",
    "\n",
    "        check_x_ls = check_anomaly_in_rows(anomalies_x_dates_ls,topic,6)\n",
    "        check_y_ls = check_anomaly_in_rows(anomalies_y_dates_ls,topic,6)\n",
    "        check_z_ls = check_anomaly_in_rows(anomalies_z_dates_ls,topic,6)\n",
    "        LevelShiftAD = at_least_two_true(check_x_ls, check_y_ls, check_z_ls)\n",
    "\n",
    "        check_x_q = check_anomaly_in_rows(anomalies_x_dates_q,topic,6)\n",
    "        check_y_q = check_anomaly_in_rows(anomalies_y_dates_q,topic,6)\n",
    "        check_z_q = check_anomaly_in_rows(anomalies_z_dates_q,topic,6)\n",
    "        QuantileAD = at_least_two_true(check_x_q, check_y_q, check_z_q)\n",
    "     \n",
    "        check_x_od = check_anomaly_in_rows(anomalies_x_dates_od,topic,6)\n",
    "        check_y_od = check_anomaly_in_rows(anomalies_y_dates_od,topic,6)\n",
    "        check_z_od = check_anomaly_in_rows(anomalies_z_dates_od,topic,6)\n",
    "        OutlierDetection = at_least_two_true(check_x_od, check_y_od, check_z_od)\n",
    "\n",
    "        return at_least_two_true(LevelShiftAD,QuantileAD,OutlierDetection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04457c06-e325-4070-8be9-92a2109e55a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_17804\\4200100245.py:14: DeprecationWarning: Callback API version 1 is deprecated, update to latest version\n",
      "  client = mqtt_client.Client(mqtt_client.CallbackAPIVersion.VERSION1,client_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MQTT Broker!\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "o gajo esta fixe\n",
      "120\n",
      "ALERTA O GAJO LEVANTOU - SE !!!!!\n"
     ]
    }
   ],
   "source": [
    "broker = 'broker.emqx.io'\n",
    "port = 1883\n",
    "topic = \"move_id/AA\"\n",
    "client_id = ''\n",
    "topic_clear = sanitize_topic_name(topic)\n",
    "\n",
    "def connect_mqtt() -> mqtt_client:\n",
    "    def on_connect(client, userdata, flags, rc):\n",
    "        if rc == 0:\n",
    "            print(\"Connected to MQTT Broker!\")\n",
    "        else:\n",
    "            print(\"Failed to connect, return code %d\\n\", rc)\n",
    "\n",
    "    client = mqtt_client.Client(mqtt_client.CallbackAPIVersion.VERSION1,client_id)\n",
    "    client.on_connect = on_connect\n",
    "    client.connect(broker, port)\n",
    "    return client\n",
    "\n",
    "\n",
    "def subscribe(client: mqtt_client):\n",
    "    #os.remove(f\"data_csv_{topic_clear}.csv\")\n",
    "    def on_message(client, userdata, msg): \n",
    "        \n",
    "        time = datetime.now()\n",
    "        data = json.loads(msg.payload.decode())\n",
    "        x = float(data['accelerometerSensor']['x'])\n",
    "        y = float(data['accelerometerSensor']['y'])\n",
    "        z = float(data['accelerometerSensor']['z'])\n",
    "        row = [time, x, y, z]\n",
    "\n",
    "        lendef = on_message_data_append(topic_clear,row)\n",
    "        print(lendef)\n",
    "        \n",
    "        if(lendef > 119):\n",
    "            if (getDetection(topic_clear,row)):\n",
    "                print(\"ALERTA O GAJO LEVANTOU - SE !!!!!\")\n",
    "            else:\n",
    "                print(\"o gajo esta fixe\")\n",
    "        \n",
    "        \n",
    "    client.subscribe(topic)\n",
    "    client.on_message = on_message\n",
    "\n",
    "\n",
    "def run():\n",
    "    client = connect_mqtt()\n",
    "    subscribe(client)\n",
    "    client.loop_forever()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fcccd-2315-4931-8840-3da81e96ffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc030a1-712c-4acd-9dfa-2a122a4dc059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
